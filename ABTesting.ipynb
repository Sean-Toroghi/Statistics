{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5VL4fg3xvkMZo32atSvxK"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Distilled AB Testing</h1>"
      ],
      "metadata": {
        "id": "FchbszUtpSTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__References__\n",
        "\n",
        "- [Trustworthy Online Controlled Experiments: A Practical Guide to A/B Testing](https://www.cambridge.org/core/books/trustworthy-online-controlled-experiments/D97B26382EB0EB2DC2019A7A7B518F59)\n",
        "- Udemy AB testing cource\n",
        "- [Product plan](https://www.productplan.com/glossary/aarrr-framework/)\n",
        "- [Medium article 2023](https://simran-pm.medium.com/heart-aarrr-frameworks-in-product-management-7596b6c717de)\n",
        "- Practical statistics for data scientists [link](https://www.oreilly.com/library/view/practical-statistics-for/9781491952955/)\n"
      ],
      "metadata": {
        "id": "rQvBH6sbpYyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary of AB testing"
      ],
      "metadata": {
        "id": "eMvQRzRy3Bz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Overview__\n",
        "\n",
        "In nutshell, AB testing analysis if a new design, service, or feature delivers what was expected as the goal. It goes through the same steps needed for hypothesis testing about population based on observation (sample).\n",
        "\n",
        "In AB testing, the goal is to find the impact/effect of applying a change to the system. This change could be defined in a range of applications, from implementing a new feature to modify a section of a website. Similar to statistical analysis, design of an AB testing includes carefully select sample size that represent the target population, define a confidence and error thresholds, and derive a decision or identify causality based on the statical analysis method (with a high probability).\n",
        "\n"
      ],
      "metadata": {
        "id": "IM5-Uy3s3ENk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keywords and concepts of a A/B testing"
      ],
      "metadata": {
        "id": "4xix4xApp4tc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EXperiment design"
      ],
      "metadata": {
        "id": "ZHgN3hEaqQ97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some keywords:\n",
        "- Variants: different versions of a product/service\n",
        "- A/B test is a two-variant test, which consists of control and test groups\n",
        "- in case of more than 2 variants, the test is called A/B/N test\n",
        "-A/B test also called randomized controlled experiment, control experiment, split test, or even sometime A/B/N test. They all refer to the same concept/method.\n",
        "- two hypotheses:\n",
        "  - null hypothesis represents the current condition, before applying the new condition we would like to experiment.\n",
        "  - alternative hypothesis represents the opposite of null hypothesis that occur after implementing the experiment.\n",
        "- key metrics: are a set of metrics that we run the experiment to optimize them, such as click through rates (CTR), sign up rate, engagement rate, average revenue per order, and retention rates.\n",
        "- overall evaluation criteria (OEC): in case of having multiple metrics, their weighted average could be used as a single metric to be optimized. This metric is called OEC.\n",
        "- guardrail metrics: in addition to key metric/s, when running an experiment, guardrail metrics are employed to make sure the new experiment does not cause negative effect on the company.\n",
        "- randomized unit: when sampling from population and assign the samples to control and treatment groups, we need to make sure they are statistically similar. Randomization unit is the unit that a randomization process is applied to when we map population to test groups. The randomization unit needs to satisfy stable unit treatment value assumption (SUTVA). SUTVA requires experiment units not interfere with one another (be independent).\n",
        "- interference (also called spillover, leakage): it results in violation of SUTVA and as the consequence, the outcome of the A?B test will be incorrect. There are two types of interference:\n",
        "  - direct: when two units are directly connected to each other. An example is social media friends in two groups.\n",
        "  - indirect: when two units are not connected directly, but due to some shared resources they connect to each other. An example in an Uber or similar platforms, users in control and treatment groups could share same pool of drivers.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "__Steps of AB testing__\n",
        "\n",
        "- pre-requisites\n",
        "  - Define key metrics (also called overall evaluation criterions (OCT))\n",
        "  - Define feasible changes, not to hard to implement that even if the positive outcome could not justify the implementation.\n",
        "  - could access to adequate randomization units for each variant. Otherwise, the outcome could not be generalized to the population. Basically define sample size according the design requirements (confidence level, max variance, ...)\n",
        "\n",
        "- Experiment design\n",
        "  - define a non-bias target population to sample from\n",
        "  - define sample size that could deliver the needed statistical power\n",
        "  - define duration while considering seasonality, primary and novelty effect, day of the week/time of the day/holidays,\n",
        "\n",
        "- Run the experiment and collect data\n",
        "\n",
        "  - this step includes setting proper instrument to log data, use third party or utilize company's own platform if available to run the experiment.\n",
        "\n",
        "\n",
        "- Results/analysis\n",
        "  - run EDA\n",
        "  - apply required pre-processing steps\n",
        "  - run analysis method on data to derive inference/prediction or other information from the collected data\n",
        "  - run sanity checks\n",
        "  - identify root causes for any issue with data during validation and sanity check process and derive lesson learned\n",
        "  - based on the results make decisions, considering the tradeoffs btw used metrics, cost of implementing the change, consider opportunity costs, compare benefits with costs based on pre-defined practical signification boundary.\n",
        "\n",
        "- Post-experiment monitoring\n",
        "  - monitor the effect of making change, since in many cases long- and short-term effects could be significantly different.\n",
        "  - Also derive lesson learned for future implementation / decision makings\n",
        "\n",
        " ---\n"
      ],
      "metadata": {
        "id": "S8x4YNiWp-Yw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Elements of a hypothesis test\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aIlCFj_Yqbg8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Confidence level & confidence interval = mean +/- margin of error__\n",
        "\n",
        "Confidence level is the probability (percentage, or certainty) that the population parameter is within a confidence interval.\n",
        "\n",
        "A 95% confidence level means than the confidence interval around the sample mean is expected to include the true mean 95% of the time.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "__Margin of error__\n",
        "\n",
        "Sample variability causes inaccuracy when computer the estimator for the target parameter. Margin of error represents how many percentage points the results are differ from the real population parameter. By adding and subtracting margin of error to/from estimator, we get the confidence interval range.\n",
        "\n",
        "A 95% confidence interval with 2 percent margin of error means the estimator is within 2 percentage points of the real population parameter 95% of the time.\n",
        "\n",
        "---\n",
        "\n",
        "__Confidence of interval__\n",
        "\n",
        "When performing statistical inference, the goal is to compute an estimator from sample, as a representative of the population parameter. Confidence interval provides a range of values in which it is likely to contain the unknown population parameter, given a sample.\n",
        "\n",
        "\n",
        "\n",
        "The width of confidence interval depends on 3 factors:\n",
        "1. variance within the population of interest\n",
        "2. the size of the sample\n",
        "3. the confidence level\n",
        "\n",
        "As the confidence level increases, the width of confidence interval also increases, to cover enough range meeting the desire confidence level.\n",
        "\n",
        "\n",
        "__Confidence level for mean:__\n",
        "$$CI = \\bar{X} \\pm t_{\\alpha /2} \\frac{s}{\\sqrt n}$$\n",
        "\n",
        "__Confidence interval for proportion:__\n",
        "$$CI = \\hat{p} \\pm z_{\\alpha / 2} \\sqrt{\\frac {p(1-p)}{n}}$$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "__Error: type I and II__\n",
        "\n",
        "\n",
        "When conducting a hypothesis testing, two types of error could occur:\n",
        "1. Type I: when we reject the null hypothesis, when it is actually true. Achieving high statistical significance avoids possibility of type I error occurrence.\n",
        "2. Type II: when we fail to reject null hypothesis, when it is actually false. Higher statistical power reduces the possibility of type II occurrence.\n",
        "\n",
        "---\n",
        "\n",
        "__P-value__\n",
        "\n",
        "P-value is the probability of obtaining at least as extreme results as we are seeing if the null hypothesis of the test is true.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "__Statistical significance__\n",
        "\n",
        "Significance level $\\alpha$ is the threshold chance of making type I error (concluding to reject null hypothesis). Statistical significance is attained when the p-value is less than the significance level.\n",
        "\n",
        "For example, statistical level of 5% enforce to reject null hypothesis only if p-value be less than 0.05. When we say we reject null hypothesis because p-value is below 0.05, it means there is less than 5% probability the alternative hypothesis holds true due to pure chance.\n",
        "\n",
        "---\n",
        "\n",
        "__Power__\n",
        "\n",
        "Statistical Power (or sensitivity) is the probability that a test correctly rejects the null hypothesis (the percentage of time the minimal effect will be detected, if such an effect exists).\n",
        "\n",
        "---\n",
        "\n",
        "__Minimum detectable effect__\n",
        "\n",
        "It the smallest change we are interested to detect in a test.\n",
        "\n",
        "---\n",
        "\n",
        "__Practical significance__\n",
        "\n",
        "In some cases we may end up with satisfactory result, despite the fact implementing such a change not be feasible. For example, we may detect a 1% positive change when conducting A/B test. However, the actual implementation could result in a cost beyond the profit that 1% improvement results in. To perform a feasible change, we need to define a practical significance when designing an A/B test.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "__Sample size__\n",
        "\n",
        "Number of unit per variant plays an important role in design an A/B test. Factors affecting sample size are minimum detectable difference, significance level and statistical power.\n",
        "\n",
        "We may also need to define _duration_ for a test, which shows for how long we need to run the test for reaching adequate sample size per variation.\n"
      ],
      "metadata": {
        "id": "-7RdP3s9qekN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Factors affect test validity"
      ],
      "metadata": {
        "id": "UMrqEv_YrZCa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Novelty effect__\n",
        "\n",
        "A short-term effect caused by a change. The main takeaway is a change could trigger curiosity of users and the effect disappears after some time. This is called novelty effect.\n",
        "\n",
        "---\n",
        "\n",
        "__Primacy effect__\n",
        "\n",
        "Primacy effect acts opposite to novelty effect. When a new change occurs, some users resist to change and resist to change. But after some time, they may start to experiment with the change and like it.\n",
        "\n",
        "---\n",
        "\n",
        "__Seasonality__\n",
        "\n",
        "Seasonality could affect users behavior. As an example, an eCommerce website does not have the same traffic throughout a year. During the shopping period such as Black Friday, they experience a peak.\n",
        "\n",
        "---\n",
        "__Day of week effect__\n",
        "\n",
        "The same way seasonality effect s users' behavior, time of week also could have effect on the users' interaction with a product or service.  For example, conversion rate is usually higher on Thursday that on the weekends.\n",
        "\n",
        "---\n",
        "\n",
        "__Business cycle__\n",
        "\n",
        "Business cycle is defined as the variation of an economic system (such as the U.S. economy) over time. It is often advised to run a test for more than one business cycle, to reduce external (environmental) effect on the outcome of the test.\n",
        "\n",
        "---\n",
        "\n",
        "__Marketing promotions__\n",
        "\n",
        "Some external effects such as running an experiment, when a marketing promotion is also launched could result in an incorrect conclusion.\n",
        "\n",
        "---\n",
        "\n",
        "__Other tests__\n",
        "\n",
        "Sometimes, running another test concurrent to the experiment in hand could affect each other.\n"
      ],
      "metadata": {
        "id": "ILUE63_lrc_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A/B testing metrics"
      ],
      "metadata": {
        "id": "E01c2V5hAKfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "__Criterio for selecting a metric__\n",
        "- Measurable within the experiment timeframe.\n",
        "- Attributable to the change in the product/feature.\n",
        "- Sensitive enough to detect changes that matter in a timely fashion.\n",
        "\n",
        "__Also metric needs to be satable and simple.__ It needs to be simple, so everyone could understand it and broadly acceptable by stakeholders. A sofisticated metric that no-one desire to put time to understand for a marginal gain does not traslate to a winning approach. Furthermore, a stable metric does not require an update or modification every time we run the test.\n",
        "\n",
        "Two types of metrics\n",
        "- goal metrics (long term, less sensitive, slow move)\n",
        "- driver metrics (short-term, higher sensitivity, fast move)\n",
        "\n"
      ],
      "metadata": {
        "id": "8yunIJQ09YYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Driver metrics"
      ],
      "metadata": {
        "id": "M3jmzLRTAQ5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "__Driver metrics__\n",
        "\n",
        "These metrics reflect hypotheses on the drivers of success and indicates we are moving in the right direction toward the goal metrics. They are actionable and resistant to gaming .Some business goal related driver metrics are growth, revenue, and engagement.\n",
        "\n",
        "Design a driver metric:\n",
        "\n",
        "  Mainly we follow user funnel approach for designing a driver metric. An example of this approch is AARRR (acquisition, activation, retention, referral, and revenue) framework, or HEART (happiness, engagement, adoption, retention, and task Success) framework.\n",
        "\n",
        "---\n",
        "\n",
        "### User funnel\n",
        "A user funnel is a pathway that a user takes when they go from lead to paying customers and beyond. It traces the customer journey from first recognizing they need a solution to a problem, becoming aware of your brand, and their entire sales process. It also covers their behavior post-purchase.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### AARRR framework (also known as Pirate Metrcis framework).\n",
        "This framework was invented by Dave McClure in 2007. It consists of five user-behavior metrics that product-led growth businesses should be tracking.\n",
        "\n",
        "__Step 1: Identify the metrics__\n",
        "\n",
        "At this step we define the five metrics:\n",
        "- Acquisition (or awareness): How are people discovering our product or ompany?\n",
        "\n",
        "  It includes all the channels a company utilizes to introduce its product/service.\n",
        "\n",
        "- Activation: Are these people taking the actions we want them to?\n",
        "\n",
        "  It investigate if customers/users take a desire action, such as sign up for a free trial.\n",
        "\n",
        "- Retention: Are our activated users continuing to engage with the roduct?\n",
        "\n",
        "  It consists of monitor those who took the previous step, and see if they continue using the product/service. An example is returning product, or sign up for permanent membership after the free trial period.\n",
        "\n",
        "- Referral: Do users like the product enough to tell others about it?\n",
        "  \n",
        "  Whether the customers take further action to introduce the product/service to others. Example is referal contest or emails with promotions embeddd.\n",
        "\n",
        "- Revenue: Are our personas willing to pay for this product?\n",
        "\n",
        "  In this section we investigate if the cost of performing all the above action plus making the product/service is turned into revenue. Examples are computing minimum revenue, or break-even revenue.\n",
        "\n",
        "__Step 2: setup process to track and analyze the metrics__\n",
        "\n",
        "After defining the metrics, we need to implement tools to collect required data. An example is KISSmetrics. Depending on company resouces, this could be done within the company of employ a third party.\n",
        "\n",
        "At this stage, we also need to determine an estimated dollar amount for each category of user behavior to evaluate effectiveness of product management and marketing initiatives.\n",
        "\n",
        "__Step 3: RUN A/B testing__\n",
        "\n",
        "Here we employ a range of A/B testing, and see if we could improve each of the five stages of the AARRR framework.\n",
        "\n",
        "__Step 4: improve according to the results__\n",
        "\n",
        "At this poind, based on the outcome of the tests ran in the previous step, we implement the best approach and adjust the marketing or product feature accordingly.\n",
        "\n",
        "---\n",
        "\n",
        "### HEART framework\n",
        "\n",
        "The HEART framework is a methodology to improve the user experience (UX) of software. It was designed at Google by Kerry Rodden. The framework helps a company evaluate any aspect of its user experience according to five user-centered metrics. It consists of five metrics:\n",
        "1. Happiness: measures how satisfied users are with the product.\n",
        "2. Engagement: measures how users interact with the product.\n",
        "3. Adoption: measures how many users adopt the product.\n",
        "4. Retention: measures how long users stay with the product.\n",
        "5. Task success: measures how well users are able to complete their tasks\n",
        "\n",
        "For each metric, we first define the followings:\n",
        "- goals: broad objectives.\n",
        "- signals: indicators that show progress toward the goals\n",
        "- metrics: consist of quantifiable data points indicating success or failure.\n",
        "\n",
        "__Example:__\n",
        "- Hapiness:\n",
        "  - Goals: increasing user satisfaction scores, reducing churn, or increasing the number of positive reviews.\n",
        "  - Signals: user feedback, survey results, or social media mentions.\n",
        "  - Metrics: Net Promoter Score (NPS), customer satisfaction (CSAT) scores, or churn rate.\n",
        "- Engagement:\n",
        "  - Goals: increasing the number of active users, increasing the average session length, or increasing the number of page views.\n",
        "  - Signals: user behavior data, such as clickstream data or session recordings.\n",
        "  - Metrics: active users, average session length, or page views per session.\n",
        "- Adoption:\n",
        "  - Goals: increasing the number of new users, increasing the number of returning users or increasing the number of users who complete a specific task.\n",
        "  - Signals: user behaviour data, such as the number of downloads or registrations.\n",
        "  - Metrics: new users, returning users, or task completion rates.\n",
        "- Retention:\n",
        "  - Goals: increasing the number of active users, increasing the average customer lifetime value (CLV), or reducing the churn rate.\n",
        "  - Signals: user behaviour data, such as the number of days since the last login or the number of times a user has opened the app.\n",
        "  - Metrics: active users, CLV, or churn rate.\n",
        "- Task success:\n",
        "  - Goals: increasing the number of tasks completed, reducing the number of errors, or increasing the user experience (UX) score.\n",
        "  - Signals: user behavior data, such as the number of tasks completed or the number of errors made.\n",
        "  - Metrics: tasks completed, errors made, or a UX score."
      ],
      "metadata": {
        "id": "N9MM87I8ASlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Guardrail metrics"
      ],
      "metadata": {
        "id": "akYVDV69ATmD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardrail metrics (counter metrics) are employed to make sure we move to sucess without violating constrains.\n",
        "\n",
        "Some examples:\n",
        "- error log\n",
        "- number of crashes\n",
        "- revenue per user\n"
      ],
      "metadata": {
        "id": "L7B2yN_WAV2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trust-related metrics"
      ],
      "metadata": {
        "id": "AXGkvoc5CXy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "They assess the trustworthiness and internal validity of experiment results.\n",
        "\n",
        "Examples:\n",
        "- have the same Sample Ratio Mismatch (SRM) guardrail across control and teratment\n",
        "\n"
      ],
      "metadata": {
        "id": "uD-2J96TCaNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Randomization units"
      ],
      "metadata": {
        "id": "SFjU3IqzCzHQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "General consideration\n",
        "- consistent user experience\n",
        "- variability\n",
        "- ethical considerations\n",
        "\n",
        "\n",
        "__Choices for randomization units__\n",
        "1. account-based\n",
        "2. cookie-based\n",
        "3. session-based\n",
        "4. IP-based\n",
        "5. device-vased\n",
        "\n",
        "\n",
        "__Randomization unit vs analyzes unit__: randomization unit is similar or coarser than analysis unit\n",
        "\n",
        "Example: randoimize unit person, analyze unit page visit\n",
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "EV5E0og8C062"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Choosing target population"
      ],
      "metadata": {
        "id": "kfwXF2e4EmQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aULMvpv_EojA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Computing sample size__\n",
        "\n",
        "The most common approach is two-sampled t-test.\n",
        "\n",
        "$\\begin{cases}  \\text{null hypothesis} H_0: \\mu_c = \\mu_t \\\\ \\text{alternative hypothesis} H_1:\\mu_c \\neq \\mu_t \\end{cases}$\n",
        "\n",
        "__Choosing sample size__ depends on the following parameters:\n",
        "1. significance level (type I error)\n",
        "2. statistical power (Type II error)\n",
        "3. variance\n",
        "4. minimally detectable error\n",
        "\n",
        "\n",
        "$$n = \\frac{(\\sigma^2_t + \\sigma^2_c) (Z_{1-\\alpha/2} + Z_{1-\\beta} )^2}{\\delta^2}$$\n",
        "\n",
        "Some most common values for the parameters:\n",
        "- $\\alpha = 0.05$\n",
        "- $\\beta = 0.2$\n",
        "- $Z_{1-\\alpha/2} = 1.96$\n",
        "- $ Z_{1-\\beta} = 0.84$\n",
        "\n",
        "Assuming we have same size treatment and control buckets:\n",
        " $$n = \\frac{16\\sigma^2}{\\delta^2}$$\n",
        " where\n",
        " - $\\sigma^2$ is the Sample variance of the difference between the Treatment and the Control (for ratio metrics, the maximum variance is 0.25)\n",
        " - $\\delta$ is practical significance (Minimum detectable effect), determined among multiple stakeholders.\n",
        "\n",
        " ---\n",
        "\n",
        " __Sanity check methods__\n",
        " 1. A/A test: it needs to be run before system is used in the application\n",
        " 2. Z-test or t-test\n",
        " 3. Chi-squared test\n"
      ],
      "metadata": {
        "id": "zppOBLjwE0CC"
      }
    }
  ]
}